\subsection{Одноклассовый метод опорных векторов (One-Class Support Vector Machine)}
Метод опорных векторов~---~набор схожих алгоритмов обучения с учителем, использующихся для задач классификации и регрессионного анализа. One-Class SVM, или одноклассовый метод опорных векторов, представляет собой специальный вариант данного семейства для поиска аномалий, позволяющий использовать его в задачах обучения без учителя. В случае, если новое измерение принадлежит классу, оно считается номинальным (система работает в номинальном режиме); если же нет, то это измерение является аномалией. Особым свойством метода опорных векторов является непрерывное уменьшение эмпирической ошибки классификации и увеличение зазора, поэтому метод также известен как \textit{метод классификатора с максимальным зазором}.

Основная идея метода~---~перевод исходных векторов из низкой размерности, где они могут быть линейно неразделимы, в пространство более высокой размерности (вплоть до бесконечной) и поиск разделяющей гиперплоскости с максимальным зазором в этом пространстве. Две параллельных гиперплоскости строятся по обеим сторонам гиперплоскости, разделяющей классы. Разделяющей гиперплоскостью будет гиперплоскость, максимизирующая расстояние до двух параллельных гиперплоскостей. Алгоритм работает в предположении, что чем больше разница или расстояние между этими параллельными гиперплоскостями, тем меньше будет средняя ошибка классификатора~\cite{LifshitsInternetAlgorithms}.

Формальное описание задачи для случая с одним классом выглядит следующим образом: дана выборка $\Omega = \left\{x_1,x_2,\dots,x_n\right\}, x_i\in {\mathbb{R}}^m$, где $x_i$~---~точка в $m$-мерном пространстве, а $y_i\in \left\{0,1\right\}$ определяет принадлежность точки классу. Строится разделяющая гиперплоскость, которая имеет вид~\eqref{eq:spec:SVM:Hyperplane}, и гиперплоскость, параллельная ей~\eqref{eq:spec:SVM:ParallelHyperplane}.

\begin{subequations}
\begin{equation} \label{eq:spec:SVM:Hyperplane}
w^T x + b = 0 \text{;}
\end{equation}
\begin{equation}\label{eq:spec:SVM:ParallelHyperplane}
w^T x + b = 1 \text{,}
\end{equation}
\end{subequations}
\begin{description}
	\item[где $w\in \mathbb{F}$]~---~перпендикуляр к разделяющей гиперплоскости;
	\item[$b\in \mathbb{R}$]~---~расстояние по модулю от гиперплоскости до начала координат.
\end{description}

Если обучающая выборка линейно разделима, то возможно выбрать гиперплоскости таким образом, чтобы между ними не лежала ни одна точка обучающей выборки, и затем максимизировать расстояние между гиперплоскостями. Ширину полосы между ними легко найти из соображений геометрии, она равна $\frac{1}{\left\|w\right\|}$~\cite{VorontsovMachineLearning}, таким образом, задача сводится к минимизации $\left\|w\right\|$. Чтобы уберечь классифиактор от переполнения зашумлёнными данными, вводятся ошибки $\xi_i$, позволяющие некоторым точкам лежать внутри зазора, и константа $C > 0$, определяющая соотношение между максимизацией зазора и количеством точек внутри него (и, соответственно, ошибкой обучения). Целевая функция с учётом этих условий имеет вид~\eqref{eq:spec:SVM:ObjFunction}~\cite{RoemerIntroToOneClassSVM}.

\begin{equation} \label{eq:spec:SVM:ObjFunction}
\begin{aligned}
&\min_{w,b,\xi_i} \frac{{\left\|w\right\|}^2}{2} + C \sum_{i=1}^{n} \xi_i \text{,} \\
&y_i(w^T \phi(x_i) + b) \geq 1 - \xi_i &\text{ для всех } i = 1,\dots,n \text{,} \\
&\xi_i \geq 0 &\text{ для всех } i = 1,\dots,n \text{.}
\end{aligned}
\end{equation}

Оригинальная версия метода опорных вектров относится к семейству алгоритмов линейной классификации, что не подходит для задачи поиска аномалий в реальных многомерных данных. Существует способ создания нелинейного классификатора, в в основе которого лежит переход от скалярных произведений к произвольным ядрам, так называемый kernel trick, позволяющий строить нелинейные разделители. Результирующий алгоритм крайне похож на алгоритм линейной классификации, с той лишь разницей, что каждое скалярное произведение в приведённых выше формулах заменяется нелинейной функцией ядра (скалярным произведением в пространстве с большей размерностью). В этом пространстве уже может существовать оптимальная разделяющая гиперплоскость. Так как размерность получаемого пространства может быть больше размерности исходного, то преобразование, сопоставляющее скалярные произведения, будет нелинейным, а значит функция, соответствующая в исходном пространстве оптимальной разделяющей гиперплоскости, будет также нелинейной~\cite{LifshitsInternetAlgorithms}. Общий вид ядра показан в формуле~\eqref{eq:spec:SVM:KernelBasic}.

\begin{equation} \label{eq:spec:SVM:KernelBasic}
K(x,x_i) = \phi(x)^T\phi(x_i)
\end{equation}

Наиболее распространённые ядра~\cite{LifshitsInternetAlgorithms}: полиномиальное однородное~\eqref{eq:spec:SVM:Kernel:PolyHomogen}, полиномиальное неоднородное~\eqref{eq:spec:SVM:Kernel:PolyNoHomogen}, радиальная базисная функция~\eqref{eq:spec:SVM:Kernel:Basis}, радиальная базисная функция Гаусса~\eqref{eq:spec:SVM:Kernel:Gauss}.

\begin{subequations}
\begin{equation} \label{eq:spec:SVM:Kernel:PolyHomogen}
K(x,x_i) = (x \cdot x_i)^d \text{;}
\end{equation}
\begin{equation} \label{eq:spec:SVM:Kernel:PolyNoHomogen}
K(x,x_i) = (x \cdot x_i + 1)^d \text{;}
\end{equation}
\begin{equation} \label{eq:spec:SVM:Kernel:Basis}
K(x,x_i) = \exp (-\gamma{\left\|x - x_i\right\|}^2), \gamma > 0 \text{;}
\end{equation}
\begin{equation} \label{eq:spec:SVM:Kernel:Gauss}
K(x,x_i) = \exp (-\frac{{\left\|x - x_i\right\|}^2}{2\sigma^2}) \text{.}
\end{equation}
\end{subequations}

Преимущества метода:
\begin{itemize}
	\item метод сводится к решению задачи квадратичного программирования, которая всегда имеет единственное решение;
	\item даёт численную оценку аномальности (расстояние от точки до гиперплоскости)~\cite{MartinCompUnsupervisedDetectionMethods}.
\end{itemize}

Недостатки:
\begin{itemize}
	\item метод неустойчив по отношению к шуму в исходных данных. Если обучающая выборка содержит шумовые выбросы, они будут существенным образом учтены при построении разделяющей гиперплоскости;
	\item необходимо вручную подбирать функцию ядра, исходя из знаний о предметной области и природе исходных данных;
	\item в общем случае, когда линейная разделимость не гарантируется, приходится подбирать управляющий параметр алгоритма $C$~\cite{VorontsovMachineLearning};
	\item в случае значительных изменений в режиме работы системы не способен обнаруживать аномалии~\cite{MartinCompUnsupervisedDetectionMethods}.
\end{itemize}